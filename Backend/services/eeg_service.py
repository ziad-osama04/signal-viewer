"""
EEG Analysis Service
====================
Pipeline (matches eeg_predictor.py exactly):
  load .npy / .csv  ‚Üí  reshape  ‚Üí  fix channels  ‚Üí  sliding window
  ‚Üí  normalize (per-channel global stats from input)
  ‚Üí  CNN predict  +  SVM predict (statistical features)
  ‚Üí  soft-vote across windows  ‚Üí  verdict  ‚Üí  return JSON + signals

Classes : ['ADFSU', 'Depression', 'REEG-PD', 'BrainLat']
Models  : eeg_model_final.keras  +  eeg_svm_model.pkl
"""

import numpy as np
import os
import traceback
from scipy.stats import skew, kurtosis as sp_kurtosis

# ‚îÄ‚îÄ Constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
WINDOW_SIZE = 992
STEP_SIZE   = 496
N_CHANNELS  = 19
CLASS_NAMES = ['ADFSU', 'Depression', 'REEG-PD', 'BrainLat']

# ‚îÄ‚îÄ Resolve models directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_THIS_DIR = os.path.dirname(os.path.abspath(__file__))

def _find_backend_root(start: str) -> str:
    current = start
    for _ in range(6):
        if os.path.isdir(os.path.join(current, "models")):
            return current
        parent = os.path.dirname(current)
        if parent == current:
            break
        current = parent
    return _THIS_DIR

MODELS_DIR = os.path.join(_find_backend_root(_THIS_DIR), "models")

# ‚îÄ‚îÄ Lazy singleton loader ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_cnn_model     = None
_svm_model     = None
_models_loaded = False

def _load_models():
    global _cnn_model, _svm_model, _models_loaded, WINDOW_SIZE, STEP_SIZE, N_CHANNELS
    if _models_loaded:
        return

    import tensorflow as tf
    import joblib

    print(f"üîç EEG models dir: {MODELS_DIR}")

    # CNN ‚Äî try both naming conventions
    for cnn_name in ("eeg_model_final.keras", "eeg_model.keras"):
        cnn_path = os.path.join(MODELS_DIR, cnn_name)
        if os.path.exists(cnn_path):
            try:
                _cnn_model = tf.keras.models.load_model(cnn_path)
                print(f"‚úÖ EEG CNN loaded  ‚Üê {cnn_path}")

                # Detect actual input shape via dummy forward pass
                # (Keras 3 removed layer.input_shape on some layer types)
                try:
                    import numpy as _np_tmp
                    dummy = _np_tmp.zeros((1, WINDOW_SIZE, N_CHANNELS, 1), dtype='float32')
                    _cnn_model.predict(dummy, verbose=0)
                    print(f"   CNN input shape : (None, {WINDOW_SIZE}, {N_CHANNELS}, 1) ‚úÖ matches")
                except Exception as shape_err:
                    err_msg = str(shape_err)
                    # Parse "expected axis -1 of input shape to have value X, but received ... shape (B, Y)"
                    import re
                    m = re.search(r'expected axis -1 of input shape to have value (\d+)', err_msg)
                    m2 = re.search(r'received input with shape \(\d+, (\d+)\)', err_msg)
                    if m and m2:
                        dense_expected = int(m.group(1))
                        dense_got      = int(m2.group(1))
                        ratio          = dense_expected / dense_got
                        WINDOW_SIZE    = int(WINDOW_SIZE * ratio)
                        STEP_SIZE      = WINDOW_SIZE // 2
                        print(f"‚ö†Ô∏è  Shape mismatch detected ‚Äî auto-correcting WINDOW_SIZE to {WINDOW_SIZE}")
                    else:
                        print(f"‚ö†Ô∏è  Could not auto-detect window size: {shape_err}")

            except Exception as e:
                print(f"‚ùå EEG CNN failed: {e}")
            break
    else:
        print(f"‚ö†Ô∏è  No CNN model found in {MODELS_DIR}")

    # SVM ‚Äî try both naming conventions
    for svm_name in ("svm_model.pkl", "eeg_svm_model.pkl"):
        svm_path = os.path.join(MODELS_DIR, svm_name)
        if os.path.exists(svm_path):
            try:
                _svm_model = joblib.load(svm_path)
                print(f"‚úÖ EEG SVM loaded  ‚Üê {svm_path}")
            except Exception as e:
                print(f"‚ùå EEG SVM failed: {e}")
            break
    else:
        print(f"‚ö†Ô∏è  No SVM model found in {MODELS_DIR}")

    _models_loaded = True


# ‚îÄ‚îÄ Step 1: Load & reshape ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _load_signal(file_path: str) -> np.ndarray:
    """Returns (T, N_CHANNELS) float32."""
    ext = os.path.splitext(file_path)[1].lower()

    if ext == ".npy":
        raw = np.load(file_path).astype(np.float32)
        print(f"üìÑ .npy raw shape: {raw.shape}")
        if raw.ndim == 3:
            raw = raw.reshape(-1, raw.shape[-1])   # (N,T,C) ‚Üí (N*T, C)
        if raw.ndim == 2 and raw.shape[0] == N_CHANNELS and raw.shape[1] != N_CHANNELS:
            raw = raw.T                             # (C, T) ‚Üí (T, C)
        if raw.ndim == 1:
            raw = raw.reshape(-1, 1)
        print(f"   Usable shape: {raw.shape}")
        return raw

    elif ext == ".csv":
        import csv
        rows = []
        with open(file_path, newline="") as f:
            for row in csv.reader(f):
                try:
                    rows.append([float(v) for v in row])
                except ValueError:
                    continue   # skip header
        raw = np.array(rows, dtype=np.float32)
        print(f"üìÑ .csv shape: {raw.shape}")
        if raw.ndim == 2 and raw.shape[1] != N_CHANNELS:
            if raw.shape[0] == N_CHANNELS:
                raw = raw.T
            else:
                raise ValueError(
                    f"Expected {N_CHANNELS} channels, got {raw.shape[1]} columns."
                )
        return raw

    else:
        raise ValueError(f"Unsupported file type: {ext}. Use .npy or .csv")


# ‚îÄ‚îÄ Step 2: Fix channels (pad or trim to N_CHANNELS) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _fix_channels(signal: np.ndarray) -> np.ndarray:
    n = signal.shape[1] if signal.ndim == 2 else 1
    if n == N_CHANNELS:
        return signal
    if n > N_CHANNELS:
        print(f"‚ö†Ô∏è  Trimming channels {n} ‚Üí {N_CHANNELS}")
        return signal[:, :N_CHANNELS]
    print(f"‚ö†Ô∏è  Padding channels {n} ‚Üí {N_CHANNELS}")
    pad = np.zeros((signal.shape[0], N_CHANNELS - n), dtype=np.float32)
    return np.hstack([signal, pad])


# ‚îÄ‚îÄ Step 3: Sliding window ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _make_windows(data: np.ndarray) -> np.ndarray:
    """Returns (N_windows, WINDOW_SIZE, N_CHANNELS)"""
    windows = []
    T = len(data)
    if T < WINDOW_SIZE:
        pad  = np.zeros((WINDOW_SIZE - T, data.shape[1]), dtype=np.float32)
        data = np.concatenate([data, pad], axis=0)
        windows.append(data[:WINDOW_SIZE])
    else:
        for start in range(0, T - WINDOW_SIZE + 1, STEP_SIZE):
            windows.append(data[start: start + WINDOW_SIZE])
    result = np.stack(windows).astype(np.float32)
    print(f"   Windows: {len(result)}  (size={WINDOW_SIZE}, step={STEP_SIZE})")
    return result


# ‚îÄ‚îÄ Step 4: Normalize ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _normalize(windows: np.ndarray) -> np.ndarray:
    """
    Per-channel global normalization across all windows and time steps.
    shape: (N, T, C) ‚Üí mean/std over axes (0,1) ‚Üí (1, 1, C)
    Matches what train_mean/train_std encode.
    Falls back gracefully if saved stats exist in models dir.
    """
    mean_path = os.path.join(MODELS_DIR, "train_mean.npy")
    std_path  = os.path.join(MODELS_DIR, "train_std.npy")

    if os.path.exists(mean_path) and os.path.exists(std_path):
        try:
            m = np.load(mean_path)
            s = np.load(std_path)
            print("‚úÖ Using saved train_mean / train_std")
            return (windows - m) / (s + 1e-8)
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load saved stats ({e}), computing from data")

    # Compute per-channel stats from this file (axis 0=windows, 1=time ‚Üí keeps C)
    m = windows.mean(axis=(0, 1), keepdims=True)          # (1, 1, C)
    s = windows.std(axis=(0, 1), keepdims=True)           # (1, 1, C)
    s = np.where(s < 1e-8, 1e-8, s)
    print("‚ÑπÔ∏è  Norm stats computed from input file (per-channel global)")
    return (windows - m) / s


# ‚îÄ‚îÄ Step 5: SVM feature extraction ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _extract_features(X: np.ndarray) -> np.ndarray:
    """
    (N, T, C) ‚Üí (N, C * N_FEATS)
    Tries 4 features/channel first (mean, std, min, max) ‚Äî matches SVM trained
    with 76 features (19 ch √ó 4).  Falls back to 8 features if SVM expects 152.
    The correct count is auto-selected to match _svm_model at call time.
    """
    N, T, C = X.shape

    # Detect how many features per channel the SVM was trained on
    try:
        expected_total = _svm_model.n_features_in_
    except AttributeError:
        # Older sklearn ‚Äî inspect the scaler inside the pipeline if available
        try:
            expected_total = _svm_model.named_steps['scaler'].n_features_in_
        except Exception:
            expected_total = C * 4   # safe default

    n_feats = expected_total // C    # features per channel

    feats = np.zeros((N, C * n_feats), dtype=np.float32)
    for i in range(N):
        col = 0
        for c in range(C):
            ch = X[i, :, c]
            # Always compute all 8 ‚Äî then slice to n_feats
            all8 = [
                float(np.mean(ch)),
                float(np.std(ch)),
                float(np.min(ch)),
                float(np.max(ch)),
                float(np.ptp(ch)),
                float(skew(ch)),
                float(sp_kurtosis(ch)),
                float(np.sqrt(np.mean(ch ** 2))),
            ]
            feats[i, col:col+n_feats] = all8[:n_feats]
            col += n_feats
    return feats


# ‚îÄ‚îÄ Step 6: CNN inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _infer_cnn(windows_norm: np.ndarray) -> dict:
    X_cnn         = np.expand_dims(windows_norm, -1)      # (N, T, C, 1)
    probs_all     = _cnn_model.predict(X_cnn, verbose=0)  # (N, n_classes)
    probs_mean    = probs_all.mean(axis=0)
    pred_idx      = int(np.argmax(probs_mean))
    window_votes  = np.argmax(probs_all, axis=1).tolist()
    agreement     = float((np.array(window_votes) == pred_idx).mean())

    return {
        'prediction':       CLASS_NAMES[pred_idx],
        'class_index':      pred_idx,
        'confidence':       float(probs_mean[pred_idx]),
        'probabilities':    {cls: float(p) for cls, p in zip(CLASS_NAMES, probs_mean)},
        'window_votes':     window_votes,
        'window_agreement': agreement,
        'model':            'CNN (Deep Learning)',
        # Legacy keys kept for frontend compatibility
        'class':            pred_idx,
        'name':             CLASS_NAMES[pred_idx],
        'probs':            {cls: float(p) for cls, p in zip(CLASS_NAMES, probs_mean)},
        'window_agree':     agreement,
        'n_windows':        len(windows_norm),
    }


# ‚îÄ‚îÄ Step 7: SVM inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _infer_svm(windows_norm: np.ndarray) -> dict:
    X_svm        = _extract_features(windows_norm)
    probs_all    = _svm_model.predict_proba(X_svm)         # (N, n_classes)
    probs_mean   = probs_all.mean(axis=0)
    pred_idx     = int(np.argmax(probs_mean))
    window_votes = np.argmax(probs_all, axis=1).tolist()
    agreement    = float((np.array(window_votes) == pred_idx).mean())

    return {
        'prediction':       CLASS_NAMES[pred_idx],
        'class_index':      pred_idx,
        'confidence':       float(probs_mean[pred_idx]),
        'probabilities':    {cls: float(p) for cls, p in zip(CLASS_NAMES, probs_mean)},
        'window_votes':     window_votes,
        'window_agreement': agreement,
        'model':            'SVM (Classical ML)',
        # Legacy keys
        'class':            pred_idx,
        'name':             CLASS_NAMES[pred_idx],
        'probs':            {cls: float(p) for cls, p in zip(CLASS_NAMES, probs_mean)},
    }


# ‚îÄ‚îÄ Step 8: Verdict ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _verdict(cnn: dict, svm: dict) -> dict:
    agree = cnn['class_index'] == svm['class_index']
    if agree:
        winner_idx  = cnn['class_index']
        winner_conf = max(cnn['confidence'], svm['confidence'])
        tiebreak    = None
    else:
        if cnn['confidence'] >= svm['confidence']:
            winner_idx, winner_conf, tiebreak = cnn['class_index'], cnn['confidence'], 'CNN'
        else:
            winner_idx, winner_conf, tiebreak = svm['class_index'], svm['confidence'], 'SVM'

    return {
        'agree':       agree,
        'prediction':  CLASS_NAMES[winner_idx],
        'class_index': winner_idx,
        'confidence':  winner_conf,
        'tiebreak':    tiebreak,
    }


# ‚îÄ‚îÄ Public entry point ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def analyze_eeg_signal(file_path: str) -> dict:
    """
    Full EEG pipeline. Returns:
    {
        analysis: { cnn: {...}, svm: {...}, verdict: {...} },
        signals:  { 'EEG_CH1': [...], ..., 'EEG_CH19': [...] },
        time:     [0, 1, 2, ...]
    }
    """
    try:
        _load_models()

        # 1. Load & reshape
        data = _load_signal(file_path)          # (T, C)

        # 2. Fix channels
        data = _fix_channels(data)              # (T, N_CHANNELS)
        T    = data.shape[0]

        # 3. Sliding windows
        windows = _make_windows(data)           # (N, WINDOW_SIZE, N_CHANNELS)

        # 4. Normalize
        windows_norm = _normalize(windows)

        # 5. Infer
        cnn_result = _infer_cnn(windows_norm) if _cnn_model is not None \
                     else {'error': 'CNN model not loaded'}
        svm_result = _infer_svm(windows_norm) if _svm_model is not None \
                     else {'error': 'SVM model not loaded'}

        # 6. Verdict (only if both models loaded)
        verdict = _verdict(cnn_result, svm_result) \
                  if _cnn_model is not None and _svm_model is not None \
                  else {'error': 'One or more models not loaded'}

        # 7. Build signals dict for viewer
        signals = {f"EEG_CH{i+1}": data[:, i].tolist() for i in range(data.shape[1])}
        time    = list(range(T))

        return {
            "analysis": {
                "cnn":     cnn_result,
                "svm":     svm_result,
                "verdict": verdict,
            },
            "signals": signals,
            "time":    time,
        }

    except Exception as e:
        tb = traceback.format_exc()
        print(f"‚ùå EEG Service crash:\n{tb}")
        return {
            "error":   "EEG Analysis Failed",
            "details": str(e),
            "trace":   tb.split('\n')[-2],
        }